{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xr1z_j4wVog"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install scikit-image\n",
        "!pip3 install diffusers\n",
        "!pip3 install spacy\n",
        "!pip3 install ftfy\n",
        "!pip3 install transformers\n",
        "!pip3 install nltk\n",
        "!pip3 install emoji\n",
        "!pip3 install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix8fhOoBDVe_",
        "outputId": "e51dbfda-08db-4257-c2f8-6003143f1a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May  9 14:19:40 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 546.12                 Driver Version: 546.12       CUDA Version: 12.3     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3090      WDDM  | 00000000:2B:00.0  On |                  N/A |\n",
            "|  0%   38C    P8              20W / 350W |    316MiB / 24576MiB |      4%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      6436    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     10312    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A     10864    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     11136    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
            "|    0   N/A  N/A     13880    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
            "|    0   N/A  N/A     14948    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
            "|    0   N/A  N/A     22488    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
            "|    0   N/A  N/A     25220    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A     26572    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     27856    C+G   ...on\\124.0.2478.80\\msedgewebview2.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22etpp-7vXH7",
        "outputId": "ece95abc-1bd2-4ddf-a649-7c406fabdbee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PC\\anaconda3\\envs\\CSE_GPU_PYTORCH\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "C:\\Users\\PC\\anaconda3\\envs\\CSE_GPU_PYTORCH\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "C:\\Users\\PC\\anaconda3\\envs\\CSE_GPU_PYTORCH\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from typing import Any, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import emoji\n",
        "import difflib\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from torch import autocast\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF5YEF9Pv-eW"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(model, input_ids):\n",
        "  return model.text_model.embeddings(input_ids)\n",
        "\n",
        "def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0):\n",
        "    bsz, tgt_len = input_ids_shape\n",
        "    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
        "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
        "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "    mask = mask.to(dtype)\n",
        "\n",
        "    if past_key_values_length > 0:\n",
        "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
        "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
        "    bsz, src_len = mask.size()\n",
        "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
        "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
        "    inverted_mask = 1.0 - expanded_mask\n",
        "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
        "\n",
        "def get_grad(model, adv_input_tokens, target, avoid, suff_start, suff_end, object_key_mask=None):\n",
        "  embedding_weight = model.get_input_embeddings().weight\n",
        "\n",
        "  one_hot = torch.zeros(\n",
        "    adv_input_tokens[suff_start:suff_end].shape[0],\n",
        "    embedding_weight.shape[0],\n",
        "    device=model.device,\n",
        "    dtype=embedding_weight.dtype\n",
        "  )\n",
        "\n",
        "  one_hot.scatter_(\n",
        "    1,\n",
        "    adv_input_tokens[suff_start:suff_end].unsqueeze(1),\n",
        "    torch.ones(one_hot.shape[0], 1, device=model.device, dtype=embedding_weight.dtype)\n",
        "  )\n",
        "  one_hot.requires_grad_()\n",
        "\n",
        "  suffix_embeds = (one_hot @ embedding_weight).unsqueeze(0)\n",
        "  embeds = get_embeddings(model, adv_input_tokens.unsqueeze(0)).detach()\n",
        "  new_embeds = torch.cat(\n",
        "      [\n",
        "          embeds[:,:suff_start,:],\n",
        "          suffix_embeds,\n",
        "          embeds[:,suff_end:,:]\n",
        "      ],\n",
        "      dim=1)\n",
        "\n",
        "  hidden_states = new_embeds\n",
        "  attention_mask = None\n",
        "\n",
        "  input_ids = adv_input_tokens.unsqueeze(0)\n",
        "  input_shape = input_ids.shape\n",
        "\n",
        "  causal_attention_mask = _make_causal_mask(input_shape, hidden_states.dtype, device=hidden_states.device)\n",
        "\n",
        "  if attention_mask is not None:\n",
        "      attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
        "\n",
        "  encoder_outputs = model.text_model.encoder(\n",
        "      inputs_embeds=hidden_states,\n",
        "      attention_mask=attention_mask,\n",
        "      causal_attention_mask=causal_attention_mask,\n",
        "      return_dict=True,\n",
        "  )\n",
        "\n",
        "\n",
        "  last_hidden_state = encoder_outputs[0]\n",
        "  last_hidden_state = model.text_model.final_layer_norm(last_hidden_state)\n",
        "\n",
        "  cosine_sim1 = F.cosine_similarity(last_hidden_state.view(-1), target, dim=0)\n",
        "  cosine_sim2 = F.cosine_similarity(last_hidden_state.view(-1), avoid, dim=0)\n",
        "\n",
        "  loss1 = 1 - cosine_sim1\n",
        "  loss2 = 1 - cosine_sim2\n",
        "\n",
        "  loss = loss1 - loss2\n",
        "  loss.backward()\n",
        "\n",
        "  return one_hot.grad.clone()\n",
        "\n",
        "def get_allowed_characters():\n",
        "    allowed_characters=['·','~','!','@','#','$','%','^','&','*','(',')','=','-','*','+','.','<','>','?',',','\\'',';',':','|','\\\\','/']\n",
        "    for i in range(ord('A'),ord('Z')+1):\n",
        "        allowed_characters.append(chr(i))\n",
        "    for i in range(0,10):\n",
        "        allowed_characters.append(str(i))\n",
        "    return allowed_characters\n",
        "\n",
        "def find_mismatches(list1, list2):\n",
        "    mismatches = []\n",
        "    for i, (elem1, elem2) in enumerate(zip(list1, list2)):\n",
        "        if elem1 != elem2:\n",
        "            mismatches.append((i, elem1, elem2))\n",
        "    return mismatches\n",
        "\n",
        "def check_encode_decode(tokenizer, tokens, MAX_LENGTH):\n",
        "    text = tokenizer.decode(tokens,skip_special_tokens=True)\n",
        "    new_tokens = tokenizer(text,return_tensors=\"pt\", padding=\"max_length\",max_length=MAX_LENGTH, truncation=True)[\"input_ids\"][0]\n",
        "    if tokens.tolist()==new_tokens.tolist():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_all_substrings(input_string):\n",
        "  s=input_string\n",
        "  start = s[0]\n",
        "  word_list = sorted(s[i:j] for i, x in enumerate(s) for j in range(i + 1, len(s) + 1) if x in start)\n",
        "  if len(word_list) > 1:\n",
        "    filtered_list = [word for word in word_list if len(word) >= len(s)//2]\n",
        "  else:\n",
        "    filtered_list = word_list\n",
        "  return filtered_list\n",
        "\n",
        "def text_has_emoji(text):\n",
        "  for character in text:\n",
        "      if character in emoji.EMOJI_DATA:\n",
        "          return True\n",
        "  return False\n",
        "\n",
        "def check_target_word(tokenizer, tokens, substrings, TARGET_WORD_RESTRICTION):\n",
        "  text = tokenizer.decode(tokens,skip_special_tokens=True)\n",
        "  words = text.split()\n",
        "  if TARGET_WORD_RESTRICTION:\n",
        "    matching_words = [word for word in words if word in substrings]\n",
        "    contains_target = any(any(word in word_item for word in substrings) for word_item in words)\n",
        "    return not (matching_words or contains_target) # or text_has_emoji(text))\n",
        "  else:\n",
        "    return True #not text_has_emoji(text)\n",
        "\n",
        "def gradient_greedy_search(T,k,B,model, input_tokens, target, avoid,\n",
        "                           suff_start, suff_end, allowed_char_idxes,\n",
        "                           tried_adv_tokens, START_THRESH,\n",
        "                           END_THRESH, SKIP_TRIED_SUFFIXES, tokenizer,\n",
        "                           MAX_LENGTH, substrings, TARGET_WORD_RESTRICTION):\n",
        "    max_sim = -np.infty #MAX\n",
        "    adv_input_tokens = input_tokens.clone()\n",
        "\n",
        "    cos_dim1 = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "    for t in range(T):\n",
        "      g = get_grad(model, adv_input_tokens, target, avoid, suff_start, suff_end)\n",
        "\n",
        "      if allowed_char_idxes is not None:\n",
        "          mask = torch.ones_like(g, dtype=torch.bool)\n",
        "          mask[:, allowed_char_idxes] = 0\n",
        "          g[mask] = np.infty\n",
        "\n",
        "      indices = (-g).topk(k).indices\n",
        "\n",
        "      new_adv_tokens = []\n",
        "      b = 0\n",
        "      while b<B:\n",
        "        adv_input_tokens_b = adv_input_tokens.clone()\n",
        "        for i in range(suff_start, suff_end):\n",
        "            if random.random() < max(END_THRESH, START_THRESH - t/T):\n",
        "              adv_input_tokens_b[i] = indices[i-suff_start][torch.randint(k, (1,))]\n",
        "\n",
        "        if SKIP_TRIED_SUFFIXES and adv_input_tokens_b in tried_adv_tokens:\n",
        "            continue\n",
        "\n",
        "        tried_adv_tokens.add(adv_input_tokens_b)\n",
        "        new_adv_tokens.append(adv_input_tokens_b)\n",
        "        b+=1\n",
        "\n",
        "      new_adv_tokens = torch.stack(new_adv_tokens)\n",
        "\n",
        "      output_embeds = None\n",
        "      with torch.no_grad():\n",
        "        output_embeds = model(new_adv_tokens)[0].view(B,-1)\n",
        "\n",
        "      target_expanded = target.unsqueeze(0).repeat(B,1)\n",
        "      avoid_expanded = avoid.unsqueeze(0).repeat(B,1)\n",
        "\n",
        "\n",
        "      cos_sim1 = cos_dim1(target_expanded, output_embeds)\n",
        "      cos_sim2 = cos_dim1(avoid_expanded, output_embeds)\n",
        "\n",
        "      cos_sim = cos_sim1 - cos_sim2\n",
        "      max_idx = torch.argmax(cos_sim,dim=0)\n",
        "\n",
        "      if max_sim < cos_sim[max_idx] and check_encode_decode(tokenizer, new_adv_tokens[max_idx], MAX_LENGTH) and check_target_word(tokenizer, new_adv_tokens[max_idx], substrings, TARGET_WORD_RESTRICTION):\n",
        "        # print(\"t:\",t, \"cos:\", cos_sim[max_idx].item(), \"adv_prompt:\", tokenizer.decode(new_adv_tokens[max_idx], clean_up_tokenization_spaces =True,skip_special_tokens=True))\n",
        "        print(\"t:\",t, \"cos:\", \"{:.3f}\".format(cos_sim[max_idx].item()), \"adv_prompt:\", tokenizer.decode(new_adv_tokens[max_idx], clean_up_tokenization_spaces =True,skip_special_tokens=True))\n",
        "        max_sim = cos_sim[max_idx]\n",
        "        adv_input_tokens = new_adv_tokens[max_idx].clone()\n",
        "\n",
        "    print(\"Done: \", tokenizer.decode(adv_input_tokens, clean_up_tokenization_spaces =True,skip_special_tokens=True))\n",
        "    return adv_input_tokens\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxXjlPtTzBL0"
      },
      "outputs": [],
      "source": [
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "device = \"cuda\"\n",
        "START_THRESH = 1\n",
        "END_THRESH = 0.20\n",
        "NUM_ADV_TOKENS = 10\n",
        "NUM_GENERATIONS = 5\n",
        "NUM_IMAGES = 7\n",
        "T = 100\n",
        "k = 256\n",
        "B_MAX = 512\n",
        "CONSTRAINED = False\n",
        "TARGET_WORD_RESTRICTION = True\n",
        "SKIP_TRIED_SUFFIXES  = True\n",
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-06)\n",
        "cos_dim1 = torch.nn.CosineSimilarity(dim=1, eps=1e-06)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198,
          "referenced_widgets": [
            "e3dd7e35d7e8478a86eb78e16e40159e",
            "1311318a73544fbf932fb6ba5ea94d78",
            "fde2fbfba1954a53852683312e1eeda6",
            "b41b0a7c371145ad82a772f0ccc58417",
            "23ced562fccb4278972ec8e3149f0884",
            "62d0782667a840379e80fe556e7dc849",
            "e4ffce7dbd1142e58591cfa6b587e337",
            "03959b354bbe4cb3b8a457736af3fd41",
            "d6c86bca05354f4585fe997f7ee76715",
            "a4a92cb677744b3cb85ebf1f4184cbde",
            "16d0d9bc594649e0b5b28949768439cd"
          ]
        },
        "id": "Z0cNKHWCwI2S",
        "outputId": "7bb28c7e-9113-4366-8b75-265d4dfac748"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading pipeline components...: 100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        }
      ],
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker = None)\n",
        "pipe = pipe.to(device)\n",
        "pipe.enable_attention_slicing()\n",
        "\n",
        "\n",
        "tokenizer = pipe.tokenizer\n",
        "pipe.text_encoder = pipe.text_encoder.float()\n",
        "model = pipe.text_encoder\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "MAX_LENGTH = tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48stFftfDVfF",
        "outputId": "3e0432fd-6c0d-45f9-e668-d92f4f73f603"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.3.0+cu118'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMQRCAaRYT2B"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  gen = torch.Generator(device=device)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  return gen.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNKDjn-PzUel"
      },
      "outputs": [],
      "source": [
        "allowed_char_idxes = None\n",
        "\n",
        "if CONSTRAINED:\n",
        "    allowed_chars = get_allowed_characters()\n",
        "    allowed_char_idxes = tokenizer(allowed_chars, return_tensors=\"pt\")[\"input_ids\"][:,1]\n",
        "\n",
        "input_target_data = []\n",
        "\n",
        "with open(\"propnoun_data_for_attacks.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        _json = json.loads(line)\n",
        "        input_target_data.append((_json[\"input_text\"],_json[\"target_text\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePpSBemHkxMl"
      },
      "outputs": [],
      "source": [
        "def find_substrings_of_target_word(input, target, mode):\n",
        "  words_A = re.split(r'\\s+|\\.', input)\n",
        "  words_B = re.split(r'\\s+|\\.', target)\n",
        "  differ = difflib.ndiff(words_A, words_B)\n",
        "  output = [word for word in differ if word.startswith('+ ')][0][2:]\n",
        "\n",
        "  if mode is not None:\n",
        "    subs = get_all_substrings(output)\n",
        "    return subs\n",
        "  else:\n",
        "    return [output]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp9e17Vqznth"
      },
      "outputs": [],
      "source": [
        "for input_text, target_text in input_target_data:\n",
        "\n",
        "    substrings = find_substrings_of_target_word(input_text.lower(), target_text.lower(), \"check\")   # mode can be None or any string for all substrings\n",
        "\n",
        "    input_tokens = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\",max_length=MAX_LENGTH, truncation=True)[\"input_ids\"].to(device)\n",
        "    target_tokens = tokenizer(target_text, return_tensors=\"pt\", padding=\"max_length\",max_length=MAX_LENGTH, truncation=True)[\"input_ids\"].to(device)\n",
        "\n",
        "    target = None\n",
        "    with torch.no_grad():\n",
        "        target = model(target_tokens)[0][0].view(-1)\n",
        "\n",
        "    avoid = None\n",
        "    with torch.no_grad():\n",
        "        avoid = model(input_tokens)[0][0].view(-1)\n",
        "\n",
        "    input_tokens = input_tokens[0]\n",
        "\n",
        "    num_adv_tokens = NUM_ADV_TOKENS\n",
        "    suff_start = len(tokenizer(input_text).input_ids)-1\n",
        "    suff_end = suff_start + num_adv_tokens\n",
        "\n",
        "    print(\"Input:\", input_text)\n",
        "    print(\"Target:\", target_text)\n",
        "    print(\"Cosine Similarity between Input and Target\", cos(avoid,target).item())\n",
        "\n",
        "    tried_adv_tokens = set()\n",
        "\n",
        "    if allowed_char_idxes is not None:\n",
        "        k = len(allowed_char_idxes)\n",
        "    B = min(k*num_adv_tokens, B_MAX)\n",
        "    all_adv_input_tokens = [gradient_greedy_search(T, k, B, model, input_tokens, target,\n",
        "                                                   avoid, suff_start, suff_end,\n",
        "                                                   allowed_char_idxes,\n",
        "                                                   tried_adv_tokens,\n",
        "                                                   START_THRESH,\n",
        "                                                   END_THRESH,\n",
        "                                                   SKIP_TRIED_SUFFIXES,\n",
        "                                                   tokenizer,\n",
        "                                                   MAX_LENGTH,\n",
        "                                                   substrings,\n",
        "                                                   TARGET_WORD_RESTRICTION\n",
        "                                                   ) for i in range(NUM_GENERATIONS)]\n",
        "\n",
        "    print(\"Number of inputs tried: \",len(tried_adv_tokens))\n",
        "\n",
        "    if allowed_char_idxes is not None:\n",
        "        print(\"Percentage of Search Space Explored:\", 100*len(tried_adv_tokens)/(len(allowed_char_idxes)**num_adv_tokens),\"%\")\n",
        "\n",
        "\n",
        "    dir_path = input_text.replace(\" \",\"_\").replace(\".\",\"_\")+\"___\"+target_text.replace(\" \",\"_\").replace(\".\",\"_\")+\"/\"\n",
        "    if os.path.exists(dir_path) and os.path.isdir(dir_path):\n",
        "      try:\n",
        "          shutil.rmtree(dir_path)\n",
        "          print(f\"Directory '{dir_path}' has been removed\")\n",
        "      except OSError as e:\n",
        "          print(f\"Error: {dir_path} : {e.strerror}\")\n",
        "\n",
        "    os.makedirs(dir_path)\n",
        "\n",
        "    adv_prompts = []\n",
        "    cnt = 1\n",
        "    for adv_input_tokens in all_adv_input_tokens:\n",
        "        final_adv_text = tokenizer.decode(adv_input_tokens, clean_up_tokenization_spaces =True,skip_special_tokens=True)\n",
        "        print(final_adv_text)\n",
        "        adv_prompts.append(emoji.demojize(final_adv_text))\n",
        "        prompt = final_adv_text\n",
        "\n",
        "        if prompt == input_text:\n",
        "            continue\n",
        "\n",
        "        images, seeds = [], []\n",
        "        for i in range(NUM_IMAGES):\n",
        "            with autocast('cuda'):\n",
        "              seed_val = int(random.random()*1000)\n",
        "              generator = set_seed(seed_val)\n",
        "              seeds.append(seed_val)\n",
        "              images += pipe(prompt, generator=generator, num_inference_steps=50).images\n",
        "        grid = image_grid(images, rows=1, cols=NUM_IMAGES)\n",
        "\n",
        "        new_dir_path = dir_path+\"prompt_\"+str(cnt)+\"/\"\n",
        "        os.makedirs(new_dir_path)\n",
        "\n",
        "        for i,image in enumerate(images):\n",
        "            image.save(new_dir_path+str(seeds[i])+\".jpg\")\n",
        "\n",
        "        grid.save(new_dir_path+\"grid_\"+str(cnt)+\".jpg\")\n",
        "        cnt = cnt + 1\n",
        "\n",
        "    prompts_dict = {\n",
        "        \"input_text\":input_text,\n",
        "        \"target_text\":target_text,\n",
        "        \"num_tokens\":NUM_ADV_TOKENS,\n",
        "        \"adv_prompts\":adv_prompts\n",
        "    }\n",
        "\n",
        "    with open(dir_path+\"prompts.json\", 'w') as file:\n",
        "        json.dump(prompts_dict, file, indent=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03959b354bbe4cb3b8a457736af3fd41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1311318a73544fbf932fb6ba5ea94d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62d0782667a840379e80fe556e7dc849",
            "placeholder": "​",
            "style": "IPY_MODEL_e4ffce7dbd1142e58591cfa6b587e337",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "16d0d9bc594649e0b5b28949768439cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23ced562fccb4278972ec8e3149f0884": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62d0782667a840379e80fe556e7dc849": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4a92cb677744b3cb85ebf1f4184cbde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b41b0a7c371145ad82a772f0ccc58417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4a92cb677744b3cb85ebf1f4184cbde",
            "placeholder": "​",
            "style": "IPY_MODEL_16d0d9bc594649e0b5b28949768439cd",
            "value": " 6/6 [00:02&lt;00:00,  1.66it/s]"
          }
        },
        "d6c86bca05354f4585fe997f7ee76715": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3dd7e35d7e8478a86eb78e16e40159e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1311318a73544fbf932fb6ba5ea94d78",
              "IPY_MODEL_fde2fbfba1954a53852683312e1eeda6",
              "IPY_MODEL_b41b0a7c371145ad82a772f0ccc58417"
            ],
            "layout": "IPY_MODEL_23ced562fccb4278972ec8e3149f0884"
          }
        },
        "e4ffce7dbd1142e58591cfa6b587e337": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fde2fbfba1954a53852683312e1eeda6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03959b354bbe4cb3b8a457736af3fd41",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6c86bca05354f4585fe997f7ee76715",
            "value": 6
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}