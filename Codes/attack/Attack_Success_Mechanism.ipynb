{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6138cc7c",
      "metadata": {
        "id": "6138cc7c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip3 install accelerate\n",
        "!pip3 install diffusers\n",
        "!pip3 install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7525b78d",
      "metadata": {
        "id": "7525b78d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import emoji\n",
        "import random\n",
        "import difflib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from itertools import combinations\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from transformers import CLIPTextModelWithProjection\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baae5b5e",
      "metadata": {
        "id": "baae5b5e",
        "outputId": "df4c033b-09aa-4c4d-f939-93d08ca4f91a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading pipeline components...: 100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.92it/s]\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker = None)\n",
        "pipe = pipe.to(device)\n",
        "pipe.enable_attention_slicing()\n",
        "\n",
        "tokenizer = pipe.tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "text_model = CLIPTextModelWithProjection.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
        "text_model.to(device)\n",
        "\n",
        "image_processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
        "image_model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6580d2f3",
      "metadata": {
        "id": "6580d2f3"
      },
      "outputs": [],
      "source": [
        "def pil_to_latent(img):\n",
        "    image = pipe.image_processor.preprocess(img).to(pipe.device).half()\n",
        "    image_latents = (\n",
        "        pipe.vae.encode(image).latent_dist.sample() * pipe.vae.config.scaling_factor\n",
        "    )\n",
        "    return image_latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39071537",
      "metadata": {
        "id": "39071537"
      },
      "outputs": [],
      "source": [
        "def get_text_embeddings(text):\n",
        "    token_ids = tokenizer(text, max_length=77, truncation=True, padding=\"max_length\",return_tensors=\"pt\").input_ids\n",
        "    return text_model(input_ids=token_ids.to(device)).text_embeds\n",
        "\n",
        "def calculate_text_embedding_shift(text_a, text_b):\n",
        "    text_a_emb = get_text_embeddings(text_a)\n",
        "    text_b_emb = get_text_embeddings(text_b)\n",
        "    cos_sim = cos(text_a_emb.view(-1), text_b_emb.view(-1))\n",
        "    return cos_sim.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394fb151",
      "metadata": {
        "id": "394fb151"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  gen = torch.Generator(device=device)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  return gen.manual_seed(seed)\n",
        "\n",
        "def generate_image(prompt, seed, filename=None, save=False):\n",
        "  gen = set_seed(seed)\n",
        "  image = pipe(prompt, num_inference_steps=20, generator=gen).images[0]\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  if save:\n",
        "    image.save(filename+\".jpg\")\n",
        "  plt.show()\n",
        "  return image\n",
        "\n",
        "def generate_image_no_save(prompt, seed):\n",
        "    gen = set_seed(seed)\n",
        "    image = pipe(prompt, num_inference_steps=20, generator=gen).images[0]\n",
        "    return image\n",
        "\n",
        "def generate_image_no_show(prompt, seed, filename=None, save=False):\n",
        "  gen = set_seed(seed)\n",
        "  image = pipe(prompt, num_inference_steps=20, generator=gen).images[0]\n",
        "  if save:\n",
        "    image.save(filename+\".jpg\")\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe9ea07",
      "metadata": {
        "id": "3fe9ea07"
      },
      "outputs": [],
      "source": [
        "def find_all_combinations(ids, placeholder):\n",
        "\n",
        "    combinations_list = []\n",
        "    for n in range(1, len(ids)): #len(ids)\n",
        "        for comb in combinations(range(len(ids)), n):\n",
        "            new_list = ids[:]\n",
        "            for index in comb:\n",
        "                new_list[index] = placeholder\n",
        "            if placeholder in new_list:\n",
        "                combinations_list.append(new_list)\n",
        "\n",
        "    return combinations_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128c8646",
      "metadata": {
        "id": "128c8646"
      },
      "outputs": [],
      "source": [
        "def calculate_image_embedding_shift(img_a, img_b):\n",
        "    img_a_emb = pil_to_latent(img_a)\n",
        "    img_b_emb = pil_to_latent(img_b)\n",
        "    sim = cos(img_a_emb.view(-1), img_b_emb.view(-1))\n",
        "    return sim.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f947ef",
      "metadata": {
        "id": "47f947ef"
      },
      "outputs": [],
      "source": [
        "def check_attack_success(img, text):\n",
        "    question = \"Does the image match the caption: \\\"{}\\\"? Answer only Yes or No?\"\n",
        "    template = question.format(text)\n",
        "    prompt = f\"[INST] <image>\\n{template} [/INST]\"\n",
        "\n",
        "    inputs = image_processor(text=prompt, images=img, return_tensors=\"pt\").to(device)\n",
        "    out = image_model.generate(**inputs, max_new_tokens=100)\n",
        "    #print(image_processor.decode(out[0], skip_special_tokens=True))\n",
        "    gen = out[:, inputs.input_ids.shape[1]:]\n",
        "    answer = image_processor.decode(gen[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    if answer == \"Yes\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3024082f",
      "metadata": {
        "id": "3024082f"
      },
      "outputs": [],
      "source": [
        "def find_target_word(input, target):\n",
        "  words_A = re.split(r'\\s+|\\.', input)\n",
        "  words_B = re.split(r'\\s+|\\.', target)\n",
        "  differ = difflib.ndiff(words_A, words_B)\n",
        "  output = [word for word in differ if word.startswith('+ ')][0][2:]\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f18adb",
      "metadata": {
        "id": "70f18adb"
      },
      "outputs": [],
      "source": [
        "def make_directory(dirs_path):\n",
        "    if os.path.exists(dirs_path) and os.path.isdir(dirs_path):\n",
        "        try:\n",
        "            shutil.rmtree(dirs_path)\n",
        "            print(f\"Directory '{dirs_path}' has been removed\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error: {dirs_path} : {e.strerror}\")\n",
        "    os.makedirs(dirs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b7c4ea",
      "metadata": {
        "id": "91b7c4ea"
      },
      "outputs": [],
      "source": [
        "def input_prompts(in_text, tar_text, suffix, seed):\n",
        "    input_text = in_text\n",
        "    target_text = tar_text\n",
        "    suffix_prompt = emoji.emojize(suffix)\n",
        "\n",
        "    print(suffix_prompt)\n",
        "\n",
        "    tokens = tokenizer.tokenize(suffix_prompt)\n",
        "    id_list = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    print(len(id_list))\n",
        "\n",
        "    target_word = find_target_word(input_text, target_text)\n",
        "    input_word  = find_target_word(target_text, input_text)\n",
        "    print(input_word, \" \", target_word)\n",
        "\n",
        "    boundary = calculate_text_embedding_shift(input_word, target_word)\n",
        "    print(\"Cosine similarity between Input word and Target Word:\", boundary)\n",
        "\n",
        "    dir_path = input_text.replace(\" \",\"_\").replace(\".\",\"_\")+\"___\"+target_text.replace(\" \",\"_\").replace(\".\",\"_\")+\"/\"\n",
        "    dir_path = \"./\" + dir_path\n",
        "    make_directory(dir_path)\n",
        "\n",
        "    input_img = generate_image(input_text, seed, dir_path+\"input_{}\".format(seed), True)\n",
        "    target_img = generate_image(target_text, seed, dir_path+\"target\", True)\n",
        "    suffix_img = generate_image(input_text+suffix_prompt, seed, dir_path+\"generated\", True)\n",
        "\n",
        "    inp_to_tar = calculate_image_embedding_shift(input_img, target_img)\n",
        "    suffix_to_tar = calculate_image_embedding_shift(suffix_img, target_img)\n",
        "    suffix_to_in = calculate_image_embedding_shift(suffix_img, input_img)\n",
        "\n",
        "    origin_text_sim = calculate_text_embedding_shift(input_text, target_text)\n",
        "    print(\"Cosine similarity between input and target text: \", origin_text_sim)\n",
        "\n",
        "    generated_text_sim = calculate_text_embedding_shift(input_text+suffix_prompt, target_text)\n",
        "    print(\"Cosine similarity between input+suffix and target text: \", generated_text_sim)\n",
        "\n",
        "    in_text_sim = calculate_text_embedding_shift(input_text+suffix_prompt, input_text)\n",
        "    print(\"Cosine similarity between input+suffix and input text: \", in_text_sim)\n",
        "\n",
        "    print(\"Cosine similarity between input and target image: \", inp_to_tar)\n",
        "    print(\"Cosine similarity between generated and target image: \",suffix_to_tar)\n",
        "    print(\"Cosine similarity between generated and input image: \",suffix_to_in)\n",
        "\n",
        "    return input_text, target_text, suffix_prompt, id_list, dir_path, input_word, target_word, input_img, target_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a2360a",
      "metadata": {
        "id": "72a2360a"
      },
      "outputs": [],
      "source": [
        "def extract_unsuccessful_combinations(input_text, target_text, input_img, target_img, id_list, dir_path, seed, placeholder):\n",
        "\n",
        "    candidates = find_all_combinations(id_list, placeholder)\n",
        "\n",
        "    cand_text_list = []\n",
        "    for comb in candidates:\n",
        "        sfx = tokenizer.decode(comb, skip_special_tokens = False)\n",
        "        cand_text = input_text +\" \"+ sfx\n",
        "        cand_text_list.append(cand_text)\n",
        "\n",
        "    comb_id_list, comb_text_list, comb_img_list, all_image = [], [], [], []\n",
        "    for idx, txt in enumerate(cand_text_list):\n",
        "\n",
        "        if idx%100 == 0:\n",
        "            print(f\"Sample {idx}\")\n",
        "\n",
        "        image = generate_image_no_save(txt, seed)\n",
        "        all_image.append(image)\n",
        "        ans = check_attack_success(image, target_text)\n",
        "        if ans == 0:\n",
        "            comb_id_list.append(candidates[idx])\n",
        "            comb_text_list.append(cand_text_list[idx])\n",
        "            comb_img_list.append(image)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    path = dir_path+\"unsuccessful_combinations/\"\n",
        "    make_directory(path)\n",
        "    for ppos in range(len(comb_text_list)):\n",
        "        tx = comb_text_list[ppos]\n",
        "        im =  comb_img_list [ppos]\n",
        "        im.save(path+f\"{ppos}.jpg\")\n",
        "\n",
        "    candidate_list, text_shift_list, image_shift_list = [], [], []\n",
        "    for idx, cand in enumerate(comb_id_list):\n",
        "        sfx = tokenizer.decode(cand, skip_special_tokens=False)\n",
        "        cand_text = input_text+sfx\n",
        "        cand_img = comb_img_list[idx]\n",
        "\n",
        "        text_sim_a = calculate_text_embedding_shift(cand_text, input_text)\n",
        "        text_sim_b = calculate_text_embedding_shift(cand_text, target_text)\n",
        "\n",
        "        image_sim_a = calculate_image_embedding_shift(cand_img, input_img)\n",
        "        image_sim_b = calculate_image_embedding_shift(cand_img, target_img)\n",
        "\n",
        "        candidate_list.append(sfx)\n",
        "        text_shift_list.append(text_sim_b - text_sim_a)\n",
        "        image_shift_list.append(image_sim_b - image_sim_a)\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df['suffix_comb'] = candidate_list\n",
        "    df['text_shift'] = text_shift_list\n",
        "    df['img_shift'] = image_shift_list\n",
        "    df.to_csv(path+\"suffix_combination.csv\", index=False)\n",
        "\n",
        "    return candidates, comb_id_list, comb_img_list, all_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a1c0ed",
      "metadata": {
        "id": "83a1c0ed"
      },
      "outputs": [],
      "source": [
        "def find_critical_tokens(input_text, target_text, id_list, comb_id_list, comb_img_list, all_image, candidates, placeholder, seed):\n",
        "\n",
        "    critical_tokens_id, non_critical_tokens_id = [], []\n",
        "    critical_token_comb_idx = -1\n",
        "    critical_tok_length = -1\n",
        "    for i in range(len(comb_id_list)):\n",
        "        critical_token_length = comb_id_list[i].count(placeholder)\n",
        "\n",
        "        critical_token_comb_idx = i\n",
        "        non_crit = comb_id_list[i]\n",
        "        crit = [placeholder]*len(id_list)\n",
        "        for idx, x in enumerate(id_list):\n",
        "            if x not in non_crit:\n",
        "                crit[idx] = x\n",
        "\n",
        "        #crit_img = all_image[crit_idx_in_comb] #generate_image_no_show(crit_text, seed)\n",
        "        crit_text = input_text + tokenizer.decode(crit, skip_special_tokens=False)\n",
        "        crit_img = generate_image_no_show(crit_text, seed)\n",
        "\n",
        "        in_succ = check_attack_success(crit_img, input_text)\n",
        "        crit_succ = check_attack_success(crit_img, target_text)\n",
        "\n",
        "        if in_succ == 0 and crit_succ == 1:\n",
        "            critical_tok_length = critical_token_length\n",
        "            critical_tokens_id = crit.copy()\n",
        "            non_critical_tokens_id = [x for x in non_crit if x != placeholder]\n",
        "            break\n",
        "\n",
        "    print(f\"Inside function: {critical_tok_length}, and {critical_tokens_id}, and {non_critical_tokens_id}\")\n",
        "    return critical_token_comb_idx, critical_tok_length, critical_tokens_id, non_critical_tokens_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410024ac",
      "metadata": {
        "id": "410024ac"
      },
      "outputs": [],
      "source": [
        "def non_critical_placeholder_check(input_text, target_text, input_img, target_img, non_critical_checklist, non_critical_tokens_id, dir_path, seed):\n",
        "\n",
        "    path = dir_path+\"non_critic_placeholder/\"\n",
        "    make_directory(path)\n",
        "    cnt = 0\n",
        "    candidate_list, text_shift_list, image_shift_list = [], [], []\n",
        "\n",
        "    for idx, tok_ids in enumerate(non_critical_checklist):\n",
        "        sfx = tokenizer.decode(tok_ids, skip_special_tokens=False)\n",
        "        cand_text = input_text + sfx\n",
        "        cand_img = generate_image_no_show(cand_text, seed, path+f\"{idx}\", True)\n",
        "\n",
        "        text_sim_a = calculate_text_embedding_shift(cand_text, input_text)\n",
        "        text_sim_b = calculate_text_embedding_shift(cand_text, target_text)\n",
        "\n",
        "        image_sim_a = calculate_image_embedding_shift(cand_img, input_img)\n",
        "        image_sim_b = calculate_image_embedding_shift(cand_img, target_img)\n",
        "\n",
        "        candidate_list.append(sfx)\n",
        "        text_shift_list.append(text_sim_b - text_sim_a)\n",
        "        image_shift_list.append(image_sim_b - image_sim_a)\n",
        "\n",
        "        in_succ = check_attack_success(cand_img, input_text)\n",
        "        tar_succ = check_attack_success(cand_img, target_text)\n",
        "        if in_succ == 0 and tar_succ == 1:\n",
        "            cnt += 1\n",
        "\n",
        "    text_shift = \"higher (as expected)\"\n",
        "    for x in text_shift_list:\n",
        "        if x < 0:\n",
        "            text_shift = \"lower\"\n",
        "            break\n",
        "\n",
        "    image_shift = \"higher (as expected)\"\n",
        "    for y in image_shift_list:\n",
        "        if y < 0:\n",
        "            image_shift = \"lower\"\n",
        "            break\n",
        "\n",
        "    match = False\n",
        "    if len(non_critical_checklist) == cnt and len(non_critical_checklist) > 0:\n",
        "        match = True\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df['candidate_suffix'] = candidate_list\n",
        "    df['text_shift'] = text_shift_list\n",
        "    df['img_shift'] = image_shift_list\n",
        "    df.to_csv(path+\"suffix_evaluation.csv\", index=False)\n",
        "\n",
        "    non_crit_tokens = []\n",
        "    if len(non_critical_tokens_id) > 0:\n",
        "        non_crit_tokens = tokenizer.convert_ids_to_tokens(non_critical_tokens_id, skip_special_tokens=True)\n",
        "\n",
        "    diction = {\n",
        "        \"tokens\": non_crit_tokens,\n",
        "        \"text_shift\": text_shift,\n",
        "        \"image_shift\": image_shift,\n",
        "        \"images_matched_target\": cnt,\n",
        "        \"simply_remove\": match\n",
        "    }\n",
        "    json_object = json.dumps(diction, indent=4)\n",
        "    with open(path+\"info.json\", \"w\") as outfile:\n",
        "        outfile.write(json_object)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b51c384",
      "metadata": {
        "id": "5b51c384"
      },
      "outputs": [],
      "source": [
        "def non_critical_removal_check(input_text, target_text, input_img, target_img, non_critical_checklist, non_critical_tokens_id, dir_path, seed):\n",
        "\n",
        "    path = dir_path+\"non_critic_removed/\"\n",
        "    make_directory(path)\n",
        "    dnt = 0\n",
        "    candidate_list, text_shift_list, image_shift_list = [], [], []\n",
        "\n",
        "    for idx, tok_ids in enumerate(non_critical_checklist):\n",
        "        sfx = tokenizer.decode(tok_ids, skip_special_tokens=True)\n",
        "        cand_text = input_text + sfx\n",
        "        cand_img = generate_image_no_show(cand_text, seed, path+f\"{idx}\", True)\n",
        "\n",
        "        text_sim_a = calculate_text_embedding_shift(cand_text, input_text)\n",
        "        text_sim_b = calculate_text_embedding_shift(cand_text, target_text)\n",
        "\n",
        "        image_sim_a = calculate_image_embedding_shift(cand_img, input_img)\n",
        "        image_sim_b = calculate_image_embedding_shift(cand_img, target_img)\n",
        "\n",
        "        candidate_list.append(sfx)\n",
        "        text_shift_list.append(text_sim_b - text_sim_a)\n",
        "        image_shift_list.append(image_sim_b - image_sim_a)\n",
        "\n",
        "        in_succ = check_attack_success(cand_img, input_text)\n",
        "        tar_succ = check_attack_success(cand_img, target_text)\n",
        "        if in_succ == 0 and tar_succ == 1:\n",
        "            dnt += 1\n",
        "\n",
        "    text_shift = \"higher (as expected)\"\n",
        "    for x in text_shift_list:\n",
        "        if x < 0:\n",
        "            text_shift = \"lower\"\n",
        "            break\n",
        "\n",
        "    image_shift = \"higher (as expected)\"\n",
        "    for y in image_shift_list:\n",
        "        if y < 0:\n",
        "            image_shift = \"lower\"\n",
        "            break\n",
        "\n",
        "    match = False\n",
        "    if len(non_critical_checklist) == dnt and len(non_critical_checklist) > 0:\n",
        "        match = True\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df['candidate_suffix'] = candidate_list\n",
        "    df['text_shift'] = text_shift_list\n",
        "    df['img_shift'] = image_shift_list\n",
        "    df.to_csv(path+\"suffix_evaluation.csv\", index=False)\n",
        "\n",
        "    non_crit_tokens = []\n",
        "    if len(non_critical_tokens_id) > 0:\n",
        "        non_crit_tokens = tokenizer.convert_ids_to_tokens(non_critical_tokens_id, skip_special_tokens=True)\n",
        "\n",
        "    diction = {\n",
        "        \"tokens\": non_crit_tokens,\n",
        "        \"text_shift\": text_shift,\n",
        "        \"image_shift\": image_shift,\n",
        "        \"images_matched_target\": dnt,\n",
        "        \"simply_remove\": match\n",
        "    }\n",
        "    json_object = json.dumps(diction, indent=4)\n",
        "    with open(path+\"info.json\", \"w\") as outfile:\n",
        "        outfile.write(json_object)\n",
        "\n",
        "    return match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7094c45",
      "metadata": {
        "id": "a7094c45"
      },
      "outputs": [],
      "source": [
        "def show_stats(input_word, target_word, id_list, comb_id_list, placeholder):\n",
        "    tokens_li = id_list\n",
        "    lists_of_tokens =  comb_id_list\n",
        "    replacement_count = {token: 0 for token in tokens_li}\n",
        "    for token_list in lists_of_tokens:\n",
        "        for i, token in enumerate(token_list):\n",
        "            if token == placeholder:\n",
        "                original_token = tokens_li[i]\n",
        "                replacement_count[original_token] += 1\n",
        "\n",
        "    freq_info = []\n",
        "    zero_occur = []\n",
        "    for token, count in replacement_count.items():\n",
        "        freq_info.append(f\"{tokenizer.convert_ids_to_tokens(token)}' was replaced by 49407 {count} times.\")\n",
        "        if count == 0:\n",
        "            zero_occur.append(token)\n",
        "    print(replacement_count)\n",
        "\n",
        "    ix = tokenizer.tokenize(input_word)[0]\n",
        "    tx = tokenizer.tokenize(target_word)[0]\n",
        "    critical_sim_list = [[], [], []]\n",
        "    for idx, tok_id in enumerate(id_list):\n",
        "        tok = tokenizer.convert_ids_to_tokens(tok_id)\n",
        "        sim = calculate_text_embedding_shift(tok, tx)\n",
        "        critical_sim_list[0].append(tok)\n",
        "        critical_sim_list[1].append(sim)\n",
        "\n",
        "    for idx, tok_id in enumerate(id_list):\n",
        "        tok = tokenizer.convert_ids_to_tokens(tok_id)\n",
        "        sim = calculate_text_embedding_shift(tok, ix)\n",
        "        critical_sim_list[2].append(sim)\n",
        "\n",
        "    ct_closeto_target = [\"No\"]*len(critical_sim_list[0])\n",
        "    for i in range(len(critical_sim_list[0])):\n",
        "        if critical_sim_list[1][i] > critical_sim_list[2][i]:\n",
        "            ct_closeto_target[i] = \"Yes\"\n",
        "\n",
        "    return freq_info, critical_sim_list, ct_closeto_target, zero_occur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d07a66",
      "metadata": {
        "id": "a0d07a66"
      },
      "outputs": [],
      "source": [
        "def main_job(inp, tar, suf, seed, placeholder):\n",
        "\n",
        "    in_txt, tar_txt, suf_txt, id_list, dir_path, in_word, tar_word, in_img, tar_img = input_prompts(inp, tar, suf, seed)\n",
        "\n",
        "    cand_id_list, comb_id_list, comb_img_list, all_img = extract_unsuccessful_combinations(in_txt, tar_txt, in_img, tar_img, id_list, dir_path, seed, placeholder)\n",
        "\n",
        "    freq_info, critical_sim_list, ct_closeto_target, zero_count = show_stats(in_word, tar_word, id_list, comb_id_list, placeholder)\n",
        "\n",
        "    crit_idx, crit_len, crit_id, non_crit_id = find_critical_tokens(in_txt, tar_txt, id_list, comb_id_list, comb_img_list, all_img, cand_id_list, placeholder, seed)\n",
        "\n",
        "    if len(zero_count) > 0:\n",
        "        if len(crit_id) == 0:\n",
        "            crit_id = [x for x in id_list if x not in zero_count]\n",
        "\n",
        "        if len(non_crit_id) > 0:\n",
        "            for itm in zero_count:\n",
        "                if itm in non_crit_id:\n",
        "                    continue\n",
        "                else:\n",
        "                    non_crit_id.append(itm)\n",
        "        else:\n",
        "            for itm in zero_count:\n",
        "                non_crit_id.append(itm)\n",
        "\n",
        "    if len(non_crit_id) == 0:\n",
        "        dictonary = {\n",
        "        \"input_text\": in_txt,\n",
        "        \"target_text\": tar_txt,\n",
        "        \"initial_suffix\": emoji.demojize(suf_txt),\n",
        "        \"frequency\": freq_info,\n",
        "        \"critical_tokens\": crit_id,\n",
        "        \"non_critical_tokens\": [],\n",
        "        \"CT_sim_target\": critical_sim_list[1],\n",
        "        \"CT_sim_input\": critical_sim_list[2],\n",
        "        \"CT_closeto_target\": ct_closeto_target\n",
        "        }\n",
        "        json_object = json.dumps(dictonary, indent=4)\n",
        "        with open(dir_path+\"info.json\", \"w\") as outfile:\n",
        "            outfile.write(json_object)\n",
        "        print(\"0 Non-crtical Tokens!\")\n",
        "\n",
        "    non_critical_checklist = []\n",
        "    indices_to_replace = [i for i, x in enumerate(id_list) if x in non_crit_id]\n",
        "    for r in range(1, len(indices_to_replace)+1):\n",
        "        for comb in combinations(indices_to_replace, r):\n",
        "            new_list = id_list[:]\n",
        "            for index in comb:\n",
        "                new_list[index] = placeholder\n",
        "            non_critical_checklist.append(new_list)\n",
        "\n",
        "    match = False\n",
        "    if len(non_crit_id) < len(id_list):\n",
        "        non_critical_placeholder_check(in_txt, tar_txt, in_img, tar_img, non_critical_checklist, non_crit_id, dir_path, seed)\n",
        "        match = non_critical_removal_check(in_txt, tar_txt, in_img, tar_img, non_critical_checklist, non_crit_id, dir_path, seed)\n",
        "\n",
        "    crit_tokens, non_crit_tokens = [], []\n",
        "    if len(crit_id) > 0:\n",
        "        crit_tokens = tokenizer.convert_ids_to_tokens(crit_id, skip_special_tokens=True)\n",
        "    if len(non_crit_id) > 0:\n",
        "        non_crit_tokens = tokenizer.convert_ids_to_tokens(non_crit_id, skip_special_tokens=True)\n",
        "\n",
        "    if len(non_crit_tokens) == 0 and len(crit_tokens) == 0:\n",
        "        crit_tokens = tokenizer.convert_ids_to_tokens(id_list, skip_special_tokens=True)\n",
        "\n",
        "    dictionary = {\n",
        "    \"input_text\": in_txt,\n",
        "    \"target_text\": tar_txt,\n",
        "    \"initial_suffix\": emoji.demojize(suf_txt),\n",
        "    \"Frequency\": freq_info,\n",
        "    \"critical_tokens\": crit_tokens,\n",
        "    \"non_critical_tokens\": non_crit_tokens,\n",
        "    \"CT_sim_target\": critical_sim_list[1],\n",
        "    \"CT_sim_input\": critical_sim_list[2],\n",
        "    \"CT_closeto_target\": ct_closeto_target,\n",
        "    \"critical_token_combination_index\": crit_idx,\n",
        "    \"number_of_critical_tokens\": len(crit_id),\n",
        "    \"remove_non_critical\": match\n",
        "    }\n",
        "\n",
        "    json_object = json.dumps(dictionary, indent=4)\n",
        "    with open(dir_path+\"summary.json\", \"w\") as outfile:\n",
        "        outfile.write(json_object)\n",
        "\n",
        "    return len(crit_tokens), len(non_crit_tokens), match, tokenizer.decode(crit_id, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d88e4e",
      "metadata": {
        "id": "c4d88e4e"
      },
      "outputs": [],
      "source": [
        "def call_main():\n",
        "\n",
        "    input_target_data = []\n",
        "    with open(\"noun_attacks.jsonl\", \"r\") as f:\n",
        "        for line in f:\n",
        "            _json = json.loads(line)\n",
        "            input_target_data.append((_json[\"input_text\"],_json[\"target_text\"], _json[\"suffix\"], _json[\"tag\"], _json[\"seed\"]))\n",
        "\n",
        "    df = {}\n",
        "    for inp, tar, suf, tag, seed in input_target_data:\n",
        "        inp = inp.lower()\n",
        "        tar = tar.lower()\n",
        "        suf = emoji.emojize(suf)\n",
        "        seed = int(seed)\n",
        "        tag = tag.lower()\n",
        "\n",
        "        crit_len, non_crit_len, match, crit_tokens = main_job(inp, tar, suf, seed, 49407)\n",
        "\n",
        "        print(crit_len, non_crit_len, match)\n",
        "        print(crit_tokens)\n",
        "        print(f\"Prompt pairs done of {tag} POS!!!\")\n",
        "\n",
        "        df['input'] = inp\n",
        "        df['target'] = tar\n",
        "        df['suffix'] = suf\n",
        "        df['tag'] = tag\n",
        "        df['seed'] = seed\n",
        "        df['crit_len'] = crit_len\n",
        "        df['non_crit_len'] = non_crit_len\n",
        "        df['remove'] = match\n",
        "        df['crit_tokens'] = crit_tokens\n",
        "\n",
        "        with open('finalized_noun_stats.jsonl', 'a') as fp:\n",
        "          json.dump(df, fp)\n",
        "          fp.write(\"\\n\")\n",
        "\n",
        "call_main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "561f337c",
      "metadata": {
        "id": "561f337c"
      },
      "outputs": [],
      "source": [
        "def critical_comb(inp_txt, tar_txt, sufx, non_cric, seed, dir_path):\n",
        "\n",
        "    sufx_toks = tokenizer.tokenize(sufx)\n",
        "    sufx_ids = tokenizer.convert_tokens_to_ids(sufx_toks)\n",
        "    non_cric_ids = tokenizer.convert_tokens_to_ids(non_cric)\n",
        "\n",
        "    inp_img = generate_image_no_show(inp_txt, seed)\n",
        "    tar_img = generate_image_no_show(tar_txt, seed)\n",
        "\n",
        "    critical_checklist = []\n",
        "    img_match = []\n",
        "\n",
        "    # Define the placeholder\n",
        "    placeholder = 49407\n",
        "\n",
        "    # Find the indices in the large list where the small list elements are located\n",
        "    indices_to_replace = [i for i, x in enumerate(sufx_ids) if x not in non_cric_ids]\n",
        "\n",
        "    # Generate all possible combinations of these indices\n",
        "    for r in range(1, len(indices_to_replace)):\n",
        "        for comb in combinations(indices_to_replace, r):\n",
        "            new_list = sufx_ids[:]\n",
        "            for index in comb:\n",
        "                new_list[index] = placeholder\n",
        "            critical_checklist.append(new_list)\n",
        "\n",
        "#     print(len(critical_checklist))\n",
        "    path = dir_path+\"critic_removed/\"\n",
        "    make_directory(path)\n",
        "    dnt = 0\n",
        "    candidate_list, text_shift_list, image_shift_list = [], [], []\n",
        "\n",
        "    for idx, crit in enumerate(critical_checklist):\n",
        "        sfx = tokenizer.decode(crit, skip_special_tokens=True)\n",
        "        crit_text = inp_txt + sfx\n",
        "        crit_img = generate_image_no_show(crit_text, seed, path+f\"{idx}\", True)\n",
        "\n",
        "        #in_succ = check_attack_success(crit_img, inp_txt)\n",
        "        crit_succ = check_attack_success(crit_img, tar_txt)\n",
        "\n",
        "        text_sim_a = calculate_text_embedding_shift(crit_text, inp_txt)\n",
        "        text_sim_b = calculate_text_embedding_shift(crit_text, tar_txt)\n",
        "\n",
        "        image_sim_a = calculate_image_embedding_shift(crit_img, inp_img)\n",
        "        image_sim_b = calculate_image_embedding_shift(crit_img, tar_img)\n",
        "\n",
        "        candidate_list.append(sfx)\n",
        "        text_shift_list.append(text_sim_b - text_sim_a)\n",
        "        image_shift_list.append(image_sim_b - image_sim_a)\n",
        "\n",
        "        if crit_succ == 0:\n",
        "            dnt += 1\n",
        "        else:\n",
        "            img_match.append(idx)\n",
        "\n",
        "    not_match = False\n",
        "    if len(img_match) == 0:\n",
        "        not_match = True\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df['candidate_suffix'] = candidate_list\n",
        "    df['text_shift'] = text_shift_list\n",
        "    df['img_shift'] = image_shift_list\n",
        "    df.to_csv(path+\"suffix_evaluation.csv\", index=False)\n",
        "\n",
        "    crit_tokens_ids = [x for x in sufx_ids if x not in non_cric_ids]\n",
        "    crit_tokens = []\n",
        "    if len(crit_tokens_ids) > 0:\n",
        "        crit_tokens = tokenizer.convert_ids_to_tokens(crit_tokens_ids, skip_special_tokens=True)\n",
        "\n",
        "    diction = {\n",
        "        \"tokens\": crit_tokens,\n",
        "        \"average text shift\": sum(text_shift_list) / len(text_shift_list),\n",
        "        \"average image shift\": sum(image_shift_list) / len(image_shift_list),\n",
        "        \"total image\": len(critical_checklist),\n",
        "        \"images not matched\": dnt,\n",
        "        \"images matched\": len(critical_checklist) - dnt,\n",
        "        \"images match id\": img_match,\n",
        "        \"simply_remove\": not_match\n",
        "    }\n",
        "    json_object = json.dumps(diction, indent=4)\n",
        "    with open(path+\"info.json\", \"w\") as outfile:\n",
        "        outfile.write(json_object)\n",
        "\n",
        "    return sum(text_shift_list) / len(text_shift_list), sum(image_shift_list) / len(image_shift_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e479ff8",
      "metadata": {
        "id": "5e479ff8"
      },
      "outputs": [],
      "source": [
        "def critical_input_prompts():\n",
        "\n",
        "    input_target_data = []\n",
        "    with open(\"noun_attacks.jsonl\", \"r\") as f:\n",
        "        for line in f:\n",
        "            _json = json.loads(line)\n",
        "            input_target_data.append((_json[\"input_text\"],_json[\"target_text\"], _json[\"suffix\"], _json[\"tag\"], _json[\"seed\"], _json[\"non_cric\"]))\n",
        "\n",
        "    t_shift, i_shift = [], []\n",
        "    for inp, tar, suf, tag, seed, non_cric in input_target_data:\n",
        "        inp = inp.lower()\n",
        "        tar = tar.lower()\n",
        "        suf = emoji.emojize(suf)\n",
        "        seed = int(seed)\n",
        "        tag = tag.lower()\n",
        "        non_cric_toks=non_cric\n",
        "\n",
        "        print(inp, tar, non_cric_toks)\n",
        "\n",
        "        dir_path = inp.replace(\" \",\"_\").replace(\".\",\"_\")+\"___\"+tar.replace(\" \",\"_\").replace(\".\",\"_\")+\"/\"\n",
        "        dir_path = \"./\" + dir_path\n",
        "        make_directory(dir_path)\n",
        "\n",
        "        t, i = critical_comb(inp, tar, suf, non_cric_toks, seed, dir_path)\n",
        "        t_shift.append(t)\n",
        "        i_shift.append(i)\n",
        "\n",
        "        print(f\"Prompt pairs done of {tag} POS!!!\")\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df['avg_text_sift'] = t_shift\n",
        "    df['avg_image_shift'] = i_shift\n",
        "    df.to_csv('verb_shifts.csv', index=False)\n",
        "\n",
        "critical_input_prompts()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}